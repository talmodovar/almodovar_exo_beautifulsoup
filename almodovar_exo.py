# -*- coding: utf-8 -*-
"""almodovar_exo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UZNv8KjYpj13CzVLpC52wMOfsAiMg9RJ
"""

import requests
from bs4 import BeautifulSoup
import re

# Scrap des articles sur la page
def scraper_articles(url_page, test_mode=True):
    # Fait genre c'est un vrai navigateur
    en_tete = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    reponse = requests.get(url_page, headers=en_tete)
    page = BeautifulSoup(reponse.text, 'html.parser')

    # Recupere les articles
    balise_main = page.find('main')
    liste_articles = balise_main.find_all('article') if balise_main else []

    # Test = que 1 artikeule
    if test_mode:
        liste_articles = [liste_articles[0]] if liste_articles else []

    for article in liste_articles:
        # Cherche la mini image
        balise_img = article.find('img')
        url_miniature = balise_img['data-lazy-src'] if balise_img else None

        # Prend les infos en haut
        meta_info = article.find('div', class_='entry-meta')
        sous_categorie = meta_info.find('span', class_='favtag').get_text().strip() if meta_info else ""
        date_str = meta_info.find('span', class_='posted-on').get_text().strip() if meta_info else ""

        # Titre et lien ouais
        balise_lien = meta_info.find('a') if meta_info else None
        titre_article = balise_lien.find('h3').get_text().strip() if balise_lien else ""
        url_article = balise_lien['href'] if balise_lien else ""

        # Petit resumer
        resume = meta_info.find('div', class_='entry-excerpt').get_text().strip() if meta_info else ""

        # Va sur la page de l'article
        reponse_article = requests.get(url_article, headers=en_tete)
        page_article = BeautifulSoup(reponse_article.text, 'html.parser')

        # Qui c'est qui a ecris ?
        auteur = "On sait pas"
        for lien_auteur in page_article.find_all('a', href=True):
            if "/auteur/" in lien_auteur['href']:
                auteur = lien_auteur.get_text().strip()
                break

        # Prend tout le texte
        bloc_contenu = page_article.find('div', class_='entry-content')
        contenu_article = bloc_contenu.get_text().strip() if bloc_contenu else ""

        # Met la date comme il faut lol
        correspondance_date = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date_str.lower())
        if correspondance_date:
            jour, mois_text, annee = correspondance_date.groups()
            mois_correspondance = {'janvier':'01','février':'02','mars':'03','avril':'04','mai':'05','juin':'06',
                                   'juillet':'07','août':'08','septembre':'09','octobre':'10','novembre':'11','décembre':'12'}
            date_formatee = f"{annee}-{mois_correspondance.get(mois_text,'01')}-{jour.zfill(2)}"
        else:
            date_formatee = date_str

        # Cherche des images dans l'article
        images_trouvees = {}
        if bloc_contenu:
            for i, image in enumerate(bloc_contenu.find_all('img'), 1):
                url_image = image.get('src') or image.get('data-lazy-src')
                if not url_image or url_image.startswith("data:image"):
                    continue
                legende = image.get('alt', '') or image.get('title', '')
                images_trouvees[f'image_{i}'] = {'url': url_image, 'caption': legende}

        # Print les trucs
        print(f"\nTitre: {titre_article}")
        print(f"Miniature: {url_miniature}")
        print(f"Sous-catégorie: {sous_categorie}")
        print(f"Résumé: {resume}")
        print(f"Date: {date_formatee}")
        print(f"Auteur: {auteur}")
        print(f"Contenu: {contenu_article[:200]}...")
        print(f"Images: {len(images_trouvees)} trouvées")
        for cle, infos_img in images_trouvees.items():
            print(f"  - {cle}:")
            print(f"      URL: {infos_img['url']}")
            print(f"      Légende: {infos_img['caption']}")

        print(f"URL: {url_article}")
        print("-" * 50)

# Va checker la page
scraper_articles("https://www.blogdumoderateur.com/web/")